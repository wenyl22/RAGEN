
# Trainig paradigm
rl_or_sft: "rl"
# System settings
system:
  cuda_visible_devices: 0
  n_gpus: 1
  multi_processing: "ray"
  vllm_attention_backend: "XFORMERS"

# Model settings
model:
  base_model: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
  experiment_name: "ragen-main-exp"
  gradient_checkpointing: true

# Training parameters
training:
  train_data_num: null
  val_data_num: 50
  micro_batch_size: 1 
  train_batch_size: 1
# Each time rollout train_batch_size different envs, that is n_rollout * train_batch_size envs rolled out in each micro step
  val_batch_size: 1
  ppo_batch_size: 1 # Actual gradient descent batch size
  max_start_length: 400
  max_response_length: 1024
  max_obs_length: 200
  max_turns: 1 # Number of turns in each episode during rollout
  rollout_tp_size: 1 #
  n_rollout: 2 # Each env in the dataset is rolled out n_rollout times
  total_epochs: 5
  temperature: 0.7
  use_sft: false
  use_kl_loss: true # this means actor KL Loss
  no_think_rl: false
  state_masking: false
  binary_reward: false
  mask_state: false
  length_penalty: false
  ref_update_steps: null # every ref_update_steps, the reference model will be updated using the latest actor model
  total_training_steps: null

# Optimization parameters
optimization:
  actor_lr: 1e-6
  critic_lr: 1e-5
  kl_coef: 0.001
  kl_loss_type: low_var_kl
  adv_estimator: gae
  gpu_memory_utilization: 0.4

# Logging settings
logging:
  mode: "['wandb']"
  log_images: true
  log_image_dir: "log/trajectory"
  log_image_step_size: 4
  log_n_image_per_batch: 32

# Trainer settings
trainer:
  val_before_train: true
  val_only: false
  default_hdfs_dir: null
  nnodes: 1
  save_freq: 100
  test_freq: 200 # 100
  project_name: "RAGEN"

# SFT settings
sft:
  env_type: "sokoban"  # or "frozenlake"
  output_dir: "models/sft"

  # Data generation parameters
  data_generation:
    data_dir: "sft/data"
    algo: "bfs"
    seed: 100000 # needs to be different from the seed in the RL config
    train_size: 1000
    test_size: 100
    bfs_max_depths: 100
    prefix: "message"
    num_processes: 16

  # Training parameters
  training:
    num_gpus: 1
    max_length: 2048
    learning_rate: 1e-4
    train_batch_size: 128
    micro_batch_size: 4
    experiment_name: "test_sft_lora"
    logger: "['console','wandb']"
    epochs: 5
    hdfs_dir: null
    validate_before_training: true
    lora_rank: 64
    lora_alpha: 32
    target_modules: "all-linear"
    enable_gradient_checkpointing: false
    base_model: "Qwen/Qwen2.5-0.5B-Instruct"
    project_name: "RAGEN"
  
  # Sokoban-specific settings
  sokoban:
    dim_x: 6
    dim_y: 6
    num_boxes: 1
    max_steps: 100
    search_depth: 30
  
  # FrozenLake-specific settings
  frozenlake:
    size: 4
    p: 0.8
    is_slippery: true
